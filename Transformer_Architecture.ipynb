{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "YpaBgeiNpZWM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads):\n",
        "    super().__init__()\n",
        "    self.self_attn = torch.nn.MultiheadAttention(embed_dim, n_heads, batch_first=True)\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "        torch.nn.Linear(embed_dim, embed_dim*4),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(embed_dim*4, embed_dim),\n",
        "    )\n",
        "\n",
        "    self.in_norm  = torch.nn.LayerNorm(embed_dim)\n",
        "    self.mlp_norm = torch.nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_norm = self.in_norm(x)\n",
        "    x = x + self.self_attn(x_norm, x_norm, x_norm)[0] # the x + is the residual connection, basocally addind identity X\n",
        "    x = x + self.mlp(self.mlp_norm(x)) # this is vanilla transformer layer.\n",
        "    return x\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self, embed_dim, n_heads, n_layers):\n",
        "    super().__init__()\n",
        "    self.net = torch.nn.Sequential(\n",
        "        *[\n",
        "            TransformerBlock(embed_dim, n_heads) for _ in range(n_layers)\n",
        "        ]\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "net = Transformer(128,8,4)\n",
        "x = torch.rand(16,10,128) # Same Dimension goes in and smae dim comes out. All that happens is it goes through a bunch of transformer layers.\n",
        "print(net(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqd6MNkJpX8q",
        "outputId": "13c7d808-189d-4703-9604-625367b618d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(net)"
      ],
      "metadata": {
        "id": "7FfmXacOpcO0",
        "outputId": "77481fa0-c287-46c1-d46a-f8db372cb9f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (net): Sequential(\n",
            "    (0): TransformerBlock(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (in_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): TransformerBlock(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (in_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): TransformerBlock(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (in_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): TransformerBlock(\n",
            "      (self_attn): MultiheadAttention(\n",
            "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "      )\n",
            "      (mlp): Sequential(\n",
            "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "      )\n",
            "      (in_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlp_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}