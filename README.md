This repo consists of two different notebooks.

The first notebook consists of some notes about Attention (self and cross), Multi Head attention, Positional embeddings. It also consist of some sample code to go along with, we also try to reconstruct a rose image by learning the position of each pixel. 
The second notebook consists of a vanilla transformer architecture with features like Multi-Head attention, self attention, multi layer perceptron, residual connections, and normalizations. A great paper to reference is the paper "Attention is all you need" developed by researchers at google in 2017 which has led to ground breaking outputs in GenAI. 
https://arxiv.org/pdf/1706.03762
